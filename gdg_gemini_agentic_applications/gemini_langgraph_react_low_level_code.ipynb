{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GDG Ahlen / Incremental design of LLM-powered agentic applications](https://www.youtube.com/watch?v=uIQlMSX5gx4)\n",
    "\n",
    "Resources:\n",
    "- [Google AI Studio](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)\n",
    "    - need to get `GOOGLE_API_KEY`\n",
    "- [Finage API](https://finage.co.uk/docs/api/us-stocks#stock-market-previous-close)\n",
    "    - need to get `FINAGE_API_KEY`\n",
    "- [LangGraph: ReAct example](https://langchain-ai.github.io/langgraph/#example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: ReAct pattern using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Gemini in LangChain and import LangGraph components\n",
    "\n",
    "Important to set Experimental model of Gemini in `ChatGoogleGenerativeAI.model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import env variables from `.env` file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check for Google GEMINI API KEY\n",
    "import os\n",
    "assert \"GOOGLE_API_KEY\" in os.environ\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    max_tokens=None,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=1,\n",
    "    top_k=1,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Set system prompt\n",
    "SYSTEM_PROMPT = SystemMessage(content=\"\"\"\n",
    "    You are investment analyst equipped with a tool which gets previous day's closing price for a stock symbol.\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict\n",
    "\n",
    "@tool\n",
    "def get_stock_data(symbol: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Get previous day's closing price for a stock symbol using Finage API\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Stock symbol (e.g. 'AAPL' for Apple)\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Response data containing previous close price and other details\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"FINAGE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"FINAGE_API_KEY not found in environment variables\")\n",
    "        \n",
    "    url = f\"https://api.finage.co.uk/agg/stock/prev-close/{symbol}\"\n",
    "    \n",
    "    params = {\n",
    "        \"apikey\": api_key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Error fetching stock data: {str(e)}\")\n",
    "\n",
    "\n",
    "tools = [get_stock_data]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LangGraph methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "# def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "#     messages = state['messages']\n",
    "#     last_message = messages[-1]\n",
    "\n",
    "#     # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "#     if last_message.tool_calls:\n",
    "#         return \"tools\"\n",
    "    \n",
    "#     # Otherwise, we stop (reply to the user)\n",
    "#     return END\n",
    "# or use method `tools_condition` imported from langgraph.prebuilt\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = llm_with_tools.invoke([SYSTEM_PROMPT] + messages)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LangGraph nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReAct pattern](./images/react-pattern.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "AGENT_NODE = \"agent\"\n",
    "TOOLS_NODE = \"tools\"\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "tools_node = ToolNode(tools)\n",
    "workflow.add_node(AGENT_NODE, call_model)\n",
    "workflow.add_node(TOOLS_NODE, tools_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "workflow.add_edge(START, AGENT_NODE)\n",
    "\n",
    "# Add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    AGENT_NODE,\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "workflow.add_edge(TOOLS_NODE, AGENT_NODE)\n",
    "\n",
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it into LangChain Runnable\n",
    "# (optionally) passing the memory when compiling the graph\n",
    "agent = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LangGraph agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The previous day's closing price for Apple (AAPL) was 235.33.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign unique id to new conversation\n",
    "THREAD_ID = 42\n",
    "\n",
    "# Use the agent\n",
    "config = {\"configurable\": {\"thread_id\": THREAD_ID}}\n",
    "final_state = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the price for Apple?\")]},\n",
    "    config\n",
    ")\n",
    "\n",
    "# Display final respone\n",
    "final_state[\"messages\"][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
