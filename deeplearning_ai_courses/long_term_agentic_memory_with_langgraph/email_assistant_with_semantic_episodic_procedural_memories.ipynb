{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Email Assistant with Semantic + Episodic + Procedural Memory\n",
    "\n",
    "We previously built an email assistant that:\n",
    "- Classifies incoming messages (respond, ignore, notify)\n",
    "- Uses human-in-the-loop to refine the assistant's ability to classify emails\n",
    "- Drafts responses\n",
    "- Schedules meetings\n",
    "- Uses memory to remember details from previous emails\n",
    "\n",
    "Now, we'll add procedural memory that allows the user to update instructions for using the calendar and email writing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from models import get_embeddings_model\n",
    "\n",
    "store = InMemoryStore(\n",
    "    index={\"embed\": get_embeddings_model()}\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"langgraph_user_id\": \"lance\"}}\n",
    "langgraph_user_id = config[\"configurable\"][\"langgraph_user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_foundation_model, get_router_model\n",
    "\n",
    "llm = get_foundation_model()\n",
    "llm_router = get_router_model(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import add_messages\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    email_input: dict\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated triage_router gets ignore, notify and respond rule from store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import format_few_shot_examples\n",
    "\n",
    "def get_email_examples(namespace: dict, state: State) -> str:\n",
    "    examples = store.search(\n",
    "        namespace, \n",
    "        query=str({\"email\": state['email_input']})\n",
    "    ) \n",
    "    return format_few_shot_examples(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ignore_prompt(namespace: dict, prompt_instructions: str, store: InMemoryStore) -> str:\n",
    "    result = store.get(namespace, \"triage_ignore\")\n",
    "    if result is None:\n",
    "        store.put(\n",
    "            namespace, \n",
    "            \"triage_ignore\", \n",
    "            {\"prompt\": prompt_instructions[\"triage_rules\"][\"ignore\"]}\n",
    "        )\n",
    "        return prompt_instructions[\"triage_rules\"][\"ignore\"]\n",
    "    else:\n",
    "        return result.value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_respond_prompt(namespace: dict, prompt_instructions: str, store: InMemoryStore) -> str:\n",
    "    result = store.get(namespace, \"triage_respond\")\n",
    "    if result is None:\n",
    "        store.put(\n",
    "            namespace, \n",
    "            \"triage_respond\", \n",
    "            {\"prompt\": prompt_instructions[\"triage_rules\"][\"respond\"]}\n",
    "        )\n",
    "        return prompt_instructions[\"triage_rules\"][\"respond\"]\n",
    "    else:\n",
    "        return result.value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notify_prompt(namespace: dict, prompt_instructions: str, store: InMemoryStore) -> str:\n",
    "    result = store.get(namespace, \"triage_notify\")\n",
    "    if result is None:\n",
    "        store.put(\n",
    "            namespace, \n",
    "            \"triage_notify\", \n",
    "            {\"prompt\": prompt_instructions[\"triage_rules\"][\"notify\"]}\n",
    "        )\n",
    "        return prompt_instructions[\"triage_rules\"][\"notify\"]\n",
    "    else:\n",
    "        return result.value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from prompts import triage_user_prompt\n",
    "from profiles import john_doe_profile as profile\n",
    "from prompts_instructions import prompt_instructions\n",
    "from prompts import triage_system_prompt\n",
    "\n",
    "def triage_router(state: State, config, store) -> Command[\n",
    "    Literal[\"response_agent\", \"__end__\"]\n",
    "]:\n",
    "    namespace = (\n",
    "        \"email_assistant\",\n",
    "        langgraph_user_id,\n",
    "        \"examples\"\n",
    "    )\n",
    "    examples = get_email_examples(config, state)\n",
    "\n",
    "    ## Changes - START ##\n",
    "    namespace = (langgraph_user_id, )\n",
    "\n",
    "    ignore_prompt = get_ignore_prompt(namespace, prompt_instructions, store)\n",
    "    notify_prompt = get_notify_prompt(namespace, prompt_instructions, store)\n",
    "    respond_prompt = get_respond_prompt(namespace, prompt_instructions, store)\n",
    "    ## Changes - END ##\n",
    "\n",
    "    system_prompt = triage_system_prompt.format(\n",
    "        full_name=profile[\"full_name\"],\n",
    "        name=profile[\"name\"],\n",
    "        user_profile_background=profile[\"user_profile_background\"],\n",
    "        ## Changes - START ##\n",
    "        triage_no=ignore_prompt,\n",
    "        triage_notify=notify_prompt,\n",
    "        triage_email=respond_prompt,\n",
    "        ## Changes - END ##\n",
    "        examples=examples\n",
    "    )\n",
    "    user_prompt = triage_user_prompt.format(\n",
    "        author=state['email_input']['author'], \n",
    "        to=state['email_input']['to'], \n",
    "        subject=state['email_input']['subject'], \n",
    "        email_thread=state['email_input']['email_thread']\n",
    "    )\n",
    "\n",
    "    result = llm_router.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if result.classification == \"respond\":\n",
    "        print(\"ðŸ“§ Classification: RESPOND - This email requires a response\")\n",
    "        goto = \"response_agent\"\n",
    "        update = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Respond to the email {state['email_input']}\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    elif result.classification == \"ignore\":\n",
    "        print(\"ðŸš« Classification: IGNORE - This email can be safely ignored\")\n",
    "        update = None\n",
    "        goto = END\n",
    "    elif result.classification == \"notify\":\n",
    "        # If real life, this would do something else\n",
    "        print(\"ðŸ”” Classification: NOTIFY - This email contains important information\")\n",
    "        update = None\n",
    "        goto = END\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid classification: {result.classification}\")\n",
    "\n",
    "    return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated get_create_prompt_func gets prompt from store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions_prompt(\n",
    "    namespace: dict, \n",
    "    prompt_instructions: str, \n",
    "    store: InMemoryStore\n",
    ") -> str:\n",
    "    result = store.get(namespace, \"agent_instructions\")\n",
    "    if result is None:\n",
    "        store.put(\n",
    "            namespace, \n",
    "            \"agent_instructions\", \n",
    "            {\"prompt\": prompt_instructions[\"agent_instructions\"]}\n",
    "        )\n",
    "        return prompt_instructions[\"agent_instructions\"]\n",
    "    else:\n",
    "        return result.value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_create_prompt_func(system_prompt, prompt_instructions, profile, langgraph_user_id, store):\n",
    "    namespace = (langgraph_user_id, )\n",
    "    prompt = get_instructions_prompt(namespace, prompt_instructions, store)\n",
    "\n",
    "    return lambda state: [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": system_prompt.format(\n",
    "                instructions=prompt, \n",
    "                **profile\n",
    "            )\n",
    "        }\n",
    "    ] + state['messages']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the email agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_system_prompt_memory = \"\"\"\n",
    "< Role >\n",
    "You are {full_name}'s executive assistant. You are a top-notch executive assistant who cares about {name} performing as well as possible.\n",
    "</ Role >\n",
    "\n",
    "< Tools >\n",
    "You have access to the following tools to help manage {name}'s communications and schedule:\n",
    "\n",
    "1. write_email(to, subject, content) - Send emails to specified recipients\n",
    "2. schedule_meeting(attendees, subject, duration_minutes, preferred_day) - Schedule calendar meetings\n",
    "3. check_calendar_availability(day) - Check available time slots for a given day\n",
    "4. manage_memory - Store any relevant information about contacts, actions, discussion, etc. in memory for future reference\n",
    "5. search_memory - Search for any relevant information that may have been stored in memory\n",
    "</ Tools >\n",
    "\n",
    "< Instructions >\n",
    "{instructions}\n",
    "</ Instructions >\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "\n",
    "manage_memory_tool = create_manage_memory_tool(\n",
    "    namespace=(\n",
    "        \"email_assistant\", \n",
    "        \"{langgraph_user_id}\",\n",
    "        \"collection\"\n",
    "    )\n",
    ")\n",
    "search_memory_tool = create_search_memory_tool(\n",
    "    namespace=(\n",
    "        \"email_assistant\",\n",
    "        \"{langgraph_user_id}\",\n",
    "        \"collection\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from tools import write_email, schedule_meeting, check_calendar_availability\n",
    "from profiles import john_doe_profile as profile\n",
    "\n",
    "tools= [\n",
    "    write_email, \n",
    "    schedule_meeting,\n",
    "    check_calendar_availability,\n",
    "    manage_memory_tool,\n",
    "    search_memory_tool\n",
    "]\n",
    "response_agent = create_react_agent(\n",
    "    # Gemini foundation model\n",
    "    llm,\n",
    "    tools=tools,\n",
    "    ## Changes - START ##\n",
    "    prompt=get_create_prompt_func(agent_system_prompt_memory, prompt_instructions, profile, langgraph_user_id, store),\n",
    "    ## Changes - END ##\n",
    "    # Use this to ensure the store is passed to the agent \n",
    "    store=store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_agent = StateGraph(State)\n",
    "email_agent = email_agent.add_node(triage_router)\n",
    "email_agent = email_agent.add_node(\"response_agent\", response_agent)\n",
    "email_agent = email_agent.add_edge(START, \"triage_router\")\n",
    "email_agent = email_agent.compile(store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Agent to update Long Term Memory in the background\n",
    "\n",
    "Your email_agent is now setup to pull its instructions from long-term memory.\n",
    "\n",
    "Now, you'll create an agent to update that memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First check current behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_input = {\n",
    "    \"author\": \"Alice Jones <alice.jones@bar.com>\",\n",
    "    \"to\": \"John Doe <john.doe@company.com>\",\n",
    "    \"subject\": \"Quick question about API documentation\",\n",
    "    \"email_thread\": \"\"\"Hi John,\n",
    "\n",
    "Urgent issue - your service is down. Is there a reason why\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“§ Classification: RESPOND - This email requires a response\n"
     ]
    }
   ],
   "source": [
    "response = email_agent.invoke(\n",
    "    {\"email_input\": email_input},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Respond to the email {'author': 'Alice Jones <alice.jones@bar.com>', 'to': 'John Doe <john.doe@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': 'Hi John,\\n\\nUrgent issue - your service is down. Is there a reason why'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, I'll respond to Alice Jones about the service being down. I will ask for more details to understand the issue better.\n",
      "Tool Calls:\n",
      "  write_email (ba2a5ca2-f9bb-490d-bf46-61d4ded720c8)\n",
      " Call ID: ba2a5ca2-f9bb-490d-bf46-61d4ded720c8\n",
      "  Args:\n",
      "    to: alice.jones@bar.com\n",
      "    content: Hi Alice,\n",
      "\n",
      "Thanks for the heads up. I'm sorry to hear about the service disruption. Could you please provide more details about the issue you are experiencing? Knowing the specific error messages or the time you encountered the problem will help us investigate this promptly.\n",
      "\n",
      "Thanks,\n",
      "John\n",
      "    subject: Re: Quick question about API documentation\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: write_email\n",
      "\n",
      "Email sent to alice.jones@bar.com with subject 'Re: Quick question about API documentation'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "OK. I've responded to Alice's email, letting her know that I received her message and asking for more details about the service disruption so we can investigate it promptly.\n"
     ]
    }
   ],
   "source": [
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and look at current values of long term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use these tools when appropriate to help manage John's tasks efficiently.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"lance\",), \"agent_instructions\").value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Direct questions from team members, meeting requests, critical bug reports'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"lance\",), \"triage_respond\").value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marketing newsletters, spam emails, mass company announcements'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"lance\",), \"triage_ignore\").value['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Team member out sick, build system notifications, project status updates'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"lance\",), \"triage_notify\").value['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Use an LLM to update instructions\n",
    "\n",
    "WARNING: below code for Prompt Optimization doesn't work with Gemini for now\n",
    "\n",
    "It fails with error:\n",
    "`ValueError: Received unsupported arguments {'method': 'json_schema'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [\n",
    "    (\n",
    "        response['messages'],\n",
    "        \"Always sign your emails `John Doe`\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\n",
    "        \"name\": \"main_agent\",\n",
    "        \"prompt\": store.get((\"lance\",), \"agent_instructions\").value['prompt'],\n",
    "        \"update_instructions\": \"keep the instructions short and to the point\",\n",
    "        \"when_to_update\": \"Update this prompt whenever there is feedback on how the agent should write emails or schedule events\"\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"triage-ignore\", \n",
    "        \"prompt\": store.get((\"lance\",), \"triage_ignore\").value['prompt'],\n",
    "        \"update_instructions\": \"keep the instructions short and to the point\",\n",
    "        \"when_to_update\": \"Update this prompt whenever there is feedback on which emails should be ignored\"\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"triage-notify\", \n",
    "        \"prompt\": store.get((\"lance\",), \"triage_notify\").value['prompt'],\n",
    "        \"update_instructions\": \"keep the instructions short and to the point\",\n",
    "        \"when_to_update\": \"Update this prompt whenever there is feedback on which emails the user should be notified of\"\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"triage-respond\", \n",
    "        \"prompt\": store.get((\"lance\",), \"triage_respond\").value['prompt'],\n",
    "        \"update_instructions\": \"keep the instructions short and to the point\",\n",
    "        \"when_to_update\": \"Update this prompt whenever there is feedback on which emails should be responded to\"\n",
    "\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received unsupported arguments {'method': 'json_schema'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_foundation_model\n\u001b[1;32m      4\u001b[0m optimizer_llm \u001b[38;5;241m=\u001b[39m get_foundation_model()\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_multi_prompt_optimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.13/site-packages/langmem/prompts/optimization.py:616\u001b[0m, in \u001b[0;36mcreate_multi_prompt_optimizer\u001b[0;34m(model, kind, config)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_multi_prompt_optimizer\u001b[39m(\n\u001b[1;32m    472\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m BaseChatModel,\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    476\u001b[0m     config: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    477\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[prompt_types\u001b[38;5;241m.\u001b[39mMultiPromptOptimizerInput, \u001b[38;5;28mlist\u001b[39m[Prompt]]:\n\u001b[1;32m    478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a multi-prompt optimizer that improves prompt effectiveness.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    This function creates an optimizer that can analyze and improve multiple prompts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMultiPromptOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.13/site-packages/langmem/prompts/optimization.py:287\u001b[0m, in \u001b[0;36mMultiPromptOptimizer.__init__\u001b[0;34m(self, model, kind, config)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Build a single-prompt optimizer used internally\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_prompt_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.13/site-packages/langmem/prompts/optimization.py:265\u001b[0m, in \u001b[0;36mcreate_prompt_optimizer\u001b[0;34m(model, kind, config)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_metaprompt_optimizer(model, config)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPromptMemoryMultiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported optimizer kind: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKINDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.13/site-packages/langmem/prompts/stateless.py:162\u001b[0m, in \u001b[0;36mPromptMemoryMultiple.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    160\u001b[0m     model \u001b[38;5;241m=\u001b[39m init_chat_model(model, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGeneralResponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_schema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1247\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.with_structured_output\u001b[0;34m(self, schema, include_raw, **kwargs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwith_structured_output\u001b[39m(\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1241\u001b[0m     schema: Union[Dict, Type[BaseModel]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m-> 1247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived unsupported arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_basemodel_subclass_safe(schema):\n\u001b[1;32m   1249\u001b[0m         parser: OutputParserLike \u001b[38;5;241m=\u001b[39m PydanticToolsParser(\n\u001b[1;32m   1250\u001b[0m             tools\u001b[38;5;241m=\u001b[39m[schema], first_tool_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Received unsupported arguments {'method': 'json_schema'}"
     ]
    }
   ],
   "source": [
    "from langmem import create_multi_prompt_optimizer\n",
    "from models import get_foundation_model\n",
    "\n",
    "optimizer_llm = get_foundation_model()\n",
    "optimizer = create_multi_prompt_optimizer(\n",
    "    optimizer_llm,\n",
    "    kind=\"prompt_memory\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = optimizer.invoke(\n",
    "    {\"trajectories\": conversations, \"prompts\": prompts}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json dumps is a bit easier to read\n",
    "import json\n",
    "print(json.dumps(updated, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
